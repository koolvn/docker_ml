version: '3'

services:
  redis:
    image: redis
    networks:
    - deployml_network

  modelserver:
    image: tensorflow/tensorflow:2.0.1-py3-jupyter
#    command: ["--gpu"]
    build: ./modelserver
    depends_on:
    - redis
    networks:
    - deployml_network
    env_file:
    - app.env
    environment:
    - SERVER_SLEEP=0.25  # Time in ms between each poll by model server against Redis
    - BATCH_SIZE=32
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        constraints:
        - node.role == worker

  webserver:
    image: koolvn/ml:webserver
    build: ./webserver
    ports:
    - "80:80"
    networks:
    - deployml_network
    depends_on:
    - redis
    env_file:
    - app.env
    environment:
    - CLIENT_SLEEP=0.25  # Time in ms between each poll by web server against Redis
    - CLIENT_MAX_TRIES=100  # Num tries by web server to retrieve results from Redis before giving up
    deploy:
      placement:
        constraints:
        - node.role == manager

  telegram_bot:
    image: koolvn/ml:tg_bot
    build: ./telegram_bot
    ports:
      - "8443:8443"
    networks:
      - deployml_network
    depends_on:
      - redis

networks:
    deployml_network:
